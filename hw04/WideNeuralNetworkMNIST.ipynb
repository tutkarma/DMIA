{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WideNeuralNetworkMNIST.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bI33c5bDECtD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Нейросеть с одним скрытым слоем на MNIST"
      ]
    },
    {
      "metadata": {
        "id": "96q3uWkSECtJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Попробуем применить модель с одним скрытым слоем на датасете MNIST. Необходимо будет реализовать функцию, обучающую модель и понять, в какое качество мы \"упираемся\"."
      ]
    },
    {
      "metadata": {
        "id": "z2mg7q3hECtL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Загрузим данные"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "yOLYQ1yQECtP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e46bc3d-4940-4281-94ce-45668e00c864"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.layers.core import Activation\n",
        "from keras.models import Sequential\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "UxOwPvI3ECtZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Для воспроизводимости расчетов воспользуемся стандартным разбиением на обучающую и тестовую выборки"
      ]
    },
    {
      "metadata": {
        "id": "ZMrTDv81ECtd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4ffd8220-ecc6-4c35-ed59-e95c4f28f6ce"
      },
      "cell_type": "code",
      "source": [
        "train, test = mnist.load_data()\n",
        "\n",
        "x_train = train[0]\n",
        "y_train = train[1]\n",
        "\n",
        "x_test = test[0]\n",
        "y_test = test[1]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yn4lXbb-ECtl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Перед тем, как начать работу, посмотрите на данные глазами"
      ]
    },
    {
      "metadata": {
        "id": "kV5dFPiNECto",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "af08fad8-7e43-429e-ad40-6bf6f3f99d33"
      },
      "cell_type": "code",
      "source": [
        "images_and_labels = list(zip(x_train,  y_train))\n",
        "for index, (image, label) in enumerate(images_and_labels[:12]):\n",
        "    plt.subplot(5, 4, index + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.title('label: %i' % label )"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAACuCAYAAACPxT46AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vohBkAF3ECtu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Преобразуем данные: сделаем так, чтобы мы работали с матрицей, у которой значения от 0 до 1"
      ]
    },
    {
      "metadata": {
        "id": "6_oqVzn6ECtx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3683db6f-5099-4fb2-8e4a-34b662419ac8"
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape(60000, 28*28)\n",
        "x_test = x_test.reshape(10000, 28*28)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XfWeY-DFECt3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "А таргет сделаем категориальной переменной (то есть значение таргета по индексу k будет говорить, является ли эта цифра k)"
      ]
    },
    {
      "metadata": {
        "id": "OTt7-JGAECt5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h3gLQqD9ECuB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Построим модель для обучения"
      ]
    },
    {
      "metadata": {
        "id": "RHk73ogYECuE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Зафиксируем гиперпараметры сети"
      ]
    },
    {
      "metadata": {
        "id": "dVLLjH_QECuF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "L6mXeFbkECuO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_accuracies_of_wide_model(units, data, batch_size=128, epochs=10, n_iterations=5):\n",
        "    \"\"\"\n",
        "    Функция создает модель с одним скрытым слоем с количеством вершин units,\n",
        "    обучается на данных data с гиперпараметрами batch_size и epochs количество\n",
        "    раз, равное n_iterations и возвращает массив метрик качества на каждой итерации.\n",
        "    \n",
        "    Для функции активации используйте relu, на последнем слое используйте softmax.\n",
        "    \n",
        "    :param units: количество вершин (регулируем)\n",
        "    :param batch_size: размер батча (берем по дефолту)\n",
        "    :param epochs: количество эпох (берем по дефолту)\n",
        "    :param n_iterations: количество итераций (берем по дефолту)\n",
        "    :param data: кортеж, (x_train, y_train, x_test, y_test)\n",
        "    :return: массив, качество на тестовой выборке по каждой итерации\n",
        "    \"\"\"\n",
        "    \n",
        "    x_train, y_train, x_test, y_test = data\n",
        "    accuracies = []\n",
        "    \n",
        "    for i in range(n_iterations):\n",
        "        model = Sequential()\n",
        "        # добавление необходимых слоев\n",
        "        model.add(Dense(units, activation=\"relu\", input_shape=x_train.shape[1:]))\n",
        "        model.add(Dense(10, activation=\"softmax\"))\n",
        "      \n",
        "        # компилируем модель\n",
        "        model.compile(optimizer=\"adam\",\n",
        "                loss='categorical_crossentropy',\n",
        "                 metrics=[\"accuracy\"])\n",
        "        \n",
        "        # обучение\n",
        "        history = model.fit(x_train, y_train,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test))\n",
        "        \n",
        "        #получение конечного качества на тестовой выборке\n",
        "        accuracy = history.history[\"acc\"][-1]\n",
        "        accuracies.append(accuracy)\n",
        "    \n",
        "    return accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yfLL28oMECuY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Переберем разное количество вершин в нашей сети. Чтобы сильно не напрягать компьютер, не берите больше 3000 вершин.\n",
        "\n",
        "Задача со звездочкой: постройте график качества и посмотрите, изменяется ли оно"
      ]
    },
    {
      "metadata": {
        "id": "APpYUJTNECuf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7435
        },
        "outputId": "d2262907-184f-4845-c9c5-76a4ee137567"
      },
      "cell_type": "code",
      "source": [
        "# делаем массив вершин (рекомендуем брать до 3000)\n",
        "units_list = [1, 1000, 2000, 3000]\n",
        "              \n",
        "data = (x_train, y_train, x_test, y_test)\n",
        "results = [get_accuracies_of_wide_model(units, data) for units in units_list]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 3s 46us/step - loss: 2.0204 - acc: 0.2064 - val_loss: 1.9139 - val_acc: 0.2139\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.8689 - acc: 0.2219 - val_loss: 1.8397 - val_acc: 0.2226\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.8147 - acc: 0.2333 - val_loss: 1.8037 - val_acc: 0.2345\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 1.7848 - acc: 0.2458 - val_loss: 1.7822 - val_acc: 0.2480\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.7644 - acc: 0.2573 - val_loss: 1.7675 - val_acc: 0.2523\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.7487 - acc: 0.2664 - val_loss: 1.7541 - val_acc: 0.2674\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 1.7358 - acc: 0.2766 - val_loss: 1.7445 - val_acc: 0.2736\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 1.7252 - acc: 0.2855 - val_loss: 1.7359 - val_acc: 0.2868\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 1.7161 - acc: 0.2951 - val_loss: 1.7293 - val_acc: 0.2933\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 1.7082 - acc: 0.3038 - val_loss: 1.7236 - val_acc: 0.2970\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 2.0427 - acc: 0.2055 - val_loss: 1.9178 - val_acc: 0.2134\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 1.8745 - acc: 0.2158 - val_loss: 1.8499 - val_acc: 0.2130\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 1.8257 - acc: 0.2192 - val_loss: 1.8189 - val_acc: 0.2209\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.7988 - acc: 0.2260 - val_loss: 1.8005 - val_acc: 0.2274\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 1.7801 - acc: 0.2369 - val_loss: 1.7846 - val_acc: 0.2428\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 1.7652 - acc: 0.2507 - val_loss: 1.7740 - val_acc: 0.2581\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 1.7532 - acc: 0.2646 - val_loss: 1.7631 - val_acc: 0.2704\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 1.7429 - acc: 0.2760 - val_loss: 1.7546 - val_acc: 0.2768\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 1.7346 - acc: 0.2826 - val_loss: 1.7485 - val_acc: 0.2882\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 1.7276 - acc: 0.2907 - val_loss: 1.7436 - val_acc: 0.2856\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 2.0549 - acc: 0.1852 - val_loss: 1.9246 - val_acc: 0.2020\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.8696 - acc: 0.2226 - val_loss: 1.8309 - val_acc: 0.2379\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.7989 - acc: 0.2574 - val_loss: 1.7829 - val_acc: 0.2718\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.7570 - acc: 0.2833 - val_loss: 1.7516 - val_acc: 0.2903\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.7278 - acc: 0.2984 - val_loss: 1.7312 - val_acc: 0.2944\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.7055 - acc: 0.3099 - val_loss: 1.7097 - val_acc: 0.3135\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.6877 - acc: 0.3195 - val_loss: 1.6959 - val_acc: 0.3221\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.6723 - acc: 0.3275 - val_loss: 1.6827 - val_acc: 0.3310\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.6584 - acc: 0.3398 - val_loss: 1.6696 - val_acc: 0.3390\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.6457 - acc: 0.3501 - val_loss: 1.6564 - val_acc: 0.3532\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 2.1217 - acc: 0.1978 - val_loss: 2.0405 - val_acc: 0.2119\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 2.0231 - acc: 0.2170 - val_loss: 1.9983 - val_acc: 0.2247\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.9745 - acc: 0.2305 - val_loss: 1.9439 - val_acc: 0.2390\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.9337 - acc: 0.2429 - val_loss: 1.9086 - val_acc: 0.2529\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.8878 - acc: 0.2606 - val_loss: 1.8464 - val_acc: 0.2644\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.8295 - acc: 0.2678 - val_loss: 1.7919 - val_acc: 0.2746\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.7839 - acc: 0.2710 - val_loss: 1.7564 - val_acc: 0.2723\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.7544 - acc: 0.2760 - val_loss: 1.7328 - val_acc: 0.2735\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.7361 - acc: 0.2809 - val_loss: 1.7203 - val_acc: 0.2863\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.7234 - acc: 0.2873 - val_loss: 1.7091 - val_acc: 0.2836\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 2.0983 - acc: 0.1828 - val_loss: 1.9862 - val_acc: 0.2163\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.9411 - acc: 0.2224 - val_loss: 1.8958 - val_acc: 0.2184\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.8721 - acc: 0.2490 - val_loss: 1.8467 - val_acc: 0.2621\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.8313 - acc: 0.2729 - val_loss: 1.8158 - val_acc: 0.2749\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.8028 - acc: 0.2910 - val_loss: 1.7937 - val_acc: 0.2976\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.7802 - acc: 0.3082 - val_loss: 1.7744 - val_acc: 0.3185\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.7581 - acc: 0.3322 - val_loss: 1.7518 - val_acc: 0.3389\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.7320 - acc: 0.3497 - val_loss: 1.7263 - val_acc: 0.3458\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.7081 - acc: 0.3523 - val_loss: 1.7113 - val_acc: 0.3409\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.6887 - acc: 0.3528 - val_loss: 1.6904 - val_acc: 0.3479\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.2368 - acc: 0.9316 - val_loss: 0.1189 - val_acc: 0.9635\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0916 - acc: 0.9727 - val_loss: 0.0840 - val_acc: 0.9742\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0572 - acc: 0.9829 - val_loss: 0.0748 - val_acc: 0.9768\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0403 - acc: 0.9879 - val_loss: 0.0656 - val_acc: 0.9776\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0271 - acc: 0.9922 - val_loss: 0.0700 - val_acc: 0.9770\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0194 - acc: 0.9947 - val_loss: 0.0653 - val_acc: 0.9802\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0150 - acc: 0.9958 - val_loss: 0.0592 - val_acc: 0.9823\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0112 - acc: 0.9973 - val_loss: 0.0626 - val_acc: 0.9823\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0086 - acc: 0.9979 - val_loss: 0.0722 - val_acc: 0.9787\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0078 - acc: 0.9981 - val_loss: 0.0777 - val_acc: 0.9805\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.2369 - acc: 0.9313 - val_loss: 0.1236 - val_acc: 0.9624\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0914 - acc: 0.9728 - val_loss: 0.0886 - val_acc: 0.9713\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0577 - acc: 0.9831 - val_loss: 0.0691 - val_acc: 0.9794\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0401 - acc: 0.9878 - val_loss: 0.0620 - val_acc: 0.9815\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0273 - acc: 0.9920 - val_loss: 0.0563 - val_acc: 0.9822\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0199 - acc: 0.9945 - val_loss: 0.0532 - val_acc: 0.9828\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0146 - acc: 0.9959 - val_loss: 0.0638 - val_acc: 0.9801\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0105 - acc: 0.9973 - val_loss: 0.0583 - val_acc: 0.9823\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0102 - acc: 0.9970 - val_loss: 0.0667 - val_acc: 0.9806\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0097 - acc: 0.9971 - val_loss: 0.0605 - val_acc: 0.9838\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.2345 - acc: 0.9323 - val_loss: 0.1155 - val_acc: 0.9656\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0907 - acc: 0.9731 - val_loss: 0.0816 - val_acc: 0.9743\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0585 - acc: 0.9822 - val_loss: 0.0738 - val_acc: 0.9769\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0391 - acc: 0.9884 - val_loss: 0.0680 - val_acc: 0.9786\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0283 - acc: 0.9918 - val_loss: 0.0591 - val_acc: 0.9820\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0203 - acc: 0.9942 - val_loss: 0.0599 - val_acc: 0.9819\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0142 - acc: 0.9960 - val_loss: 0.0634 - val_acc: 0.9807\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0100 - acc: 0.9975 - val_loss: 0.0641 - val_acc: 0.9820\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0073 - acc: 0.9984 - val_loss: 0.0622 - val_acc: 0.9824\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0124 - acc: 0.9961 - val_loss: 0.0757 - val_acc: 0.9781\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2353 - acc: 0.9331 - val_loss: 0.1164 - val_acc: 0.9640\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0917 - acc: 0.9725 - val_loss: 0.0757 - val_acc: 0.9759\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0569 - acc: 0.9837 - val_loss: 0.0673 - val_acc: 0.9784\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0378 - acc: 0.9887 - val_loss: 0.0736 - val_acc: 0.9766\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0278 - acc: 0.9917 - val_loss: 0.0597 - val_acc: 0.9814\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0209 - acc: 0.9940 - val_loss: 0.0691 - val_acc: 0.9773\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0142 - acc: 0.9960 - val_loss: 0.0689 - val_acc: 0.9794\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103 - acc: 0.9973 - val_loss: 0.0636 - val_acc: 0.9798\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0096 - acc: 0.9975 - val_loss: 0.0645 - val_acc: 0.9817\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0079 - acc: 0.9978 - val_loss: 0.0659 - val_acc: 0.9814\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.2368 - acc: 0.9319 - val_loss: 0.1217 - val_acc: 0.9624\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0917 - acc: 0.9728 - val_loss: 0.0872 - val_acc: 0.9717\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0583 - acc: 0.9824 - val_loss: 0.0729 - val_acc: 0.9747\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0402 - acc: 0.9877 - val_loss: 0.0588 - val_acc: 0.9805\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0283 - acc: 0.9915 - val_loss: 0.0670 - val_acc: 0.9788\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0186 - acc: 0.9952 - val_loss: 0.0650 - val_acc: 0.9787\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0141 - acc: 0.9962 - val_loss: 0.0690 - val_acc: 0.9793\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0119 - acc: 0.9966 - val_loss: 0.0682 - val_acc: 0.9806\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - acc: 0.9973 - val_loss: 0.0693 - val_acc: 0.9793\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0078 - acc: 0.9980 - val_loss: 0.0804 - val_acc: 0.9787\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.2125 - acc: 0.9370 - val_loss: 0.1100 - val_acc: 0.9670\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0791 - acc: 0.9753 - val_loss: 0.0791 - val_acc: 0.9748\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0500 - acc: 0.9844 - val_loss: 0.0680 - val_acc: 0.9789\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0318 - acc: 0.9900 - val_loss: 0.0675 - val_acc: 0.9788\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0223 - acc: 0.9935 - val_loss: 0.0753 - val_acc: 0.9783\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0168 - acc: 0.9950 - val_loss: 0.0636 - val_acc: 0.9824\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0147 - acc: 0.9956 - val_loss: 0.0631 - val_acc: 0.9806\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0121 - acc: 0.9962 - val_loss: 0.0802 - val_acc: 0.9785\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0124 - acc: 0.9959 - val_loss: 0.0687 - val_acc: 0.9818\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0103 - acc: 0.9968 - val_loss: 0.0742 - val_acc: 0.9798\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.2129 - acc: 0.9368 - val_loss: 0.1010 - val_acc: 0.9693\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0783 - acc: 0.9763 - val_loss: 0.0765 - val_acc: 0.9764\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0486 - acc: 0.9851 - val_loss: 0.0655 - val_acc: 0.9784\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0320 - acc: 0.9900 - val_loss: 0.0661 - val_acc: 0.9790\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0223 - acc: 0.9932 - val_loss: 0.0615 - val_acc: 0.9804\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0174 - acc: 0.9946 - val_loss: 0.0674 - val_acc: 0.9800\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0137 - acc: 0.9958 - val_loss: 0.0657 - val_acc: 0.9813\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0103 - acc: 0.9969 - val_loss: 0.0759 - val_acc: 0.9784\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0104 - acc: 0.9965 - val_loss: 0.0637 - val_acc: 0.9823\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0089 - acc: 0.9973 - val_loss: 0.0673 - val_acc: 0.9827\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.2126 - acc: 0.9373 - val_loss: 0.0972 - val_acc: 0.9704\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0804 - acc: 0.9758 - val_loss: 0.0788 - val_acc: 0.9735\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0494 - acc: 0.9844 - val_loss: 0.0725 - val_acc: 0.9779\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0317 - acc: 0.9904 - val_loss: 0.0652 - val_acc: 0.9797\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0216 - acc: 0.9935 - val_loss: 0.0657 - val_acc: 0.9800\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0182 - acc: 0.9946 - val_loss: 0.0593 - val_acc: 0.9814\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0119 - acc: 0.9968 - val_loss: 0.0738 - val_acc: 0.9781\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0123 - acc: 0.9958 - val_loss: 0.0663 - val_acc: 0.9813\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0096 - acc: 0.9973 - val_loss: 0.0751 - val_acc: 0.9791\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0101 - acc: 0.9969 - val_loss: 0.0717 - val_acc: 0.9815\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 3s 42us/step - loss: 0.2141 - acc: 0.9369 - val_loss: 0.1065 - val_acc: 0.9679\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0803 - acc: 0.9757 - val_loss: 0.0735 - val_acc: 0.9762\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0496 - acc: 0.9843 - val_loss: 0.0664 - val_acc: 0.9779\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0324 - acc: 0.9901 - val_loss: 0.0654 - val_acc: 0.9787\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0226 - acc: 0.9932 - val_loss: 0.0641 - val_acc: 0.9808\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0178 - acc: 0.9949 - val_loss: 0.0582 - val_acc: 0.9828\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0127 - acc: 0.9960 - val_loss: 0.0620 - val_acc: 0.9814\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0112 - acc: 0.9966 - val_loss: 0.0598 - val_acc: 0.9846\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0082 - acc: 0.9977 - val_loss: 0.0728 - val_acc: 0.9815\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0106 - acc: 0.9965 - val_loss: 0.0814 - val_acc: 0.9790\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 0.2134 - acc: 0.9372 - val_loss: 0.1063 - val_acc: 0.9669\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0802 - acc: 0.9758 - val_loss: 0.0817 - val_acc: 0.9747\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0486 - acc: 0.9851 - val_loss: 0.0641 - val_acc: 0.9800\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0328 - acc: 0.9898 - val_loss: 0.0596 - val_acc: 0.9811\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0226 - acc: 0.9933 - val_loss: 0.0620 - val_acc: 0.9809\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0160 - acc: 0.9949 - val_loss: 0.0608 - val_acc: 0.9830\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0124 - acc: 0.9963 - val_loss: 0.0711 - val_acc: 0.9808\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0097 - acc: 0.9971 - val_loss: 0.0753 - val_acc: 0.9779\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0125 - acc: 0.9957 - val_loss: 0.0732 - val_acc: 0.9807\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0084 - acc: 0.9974 - val_loss: 0.0811 - val_acc: 0.9790\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.2030 - acc: 0.9395 - val_loss: 0.0972 - val_acc: 0.9705\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0746 - acc: 0.9778 - val_loss: 0.0848 - val_acc: 0.9722\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0449 - acc: 0.9862 - val_loss: 0.0723 - val_acc: 0.9784\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0311 - acc: 0.9903 - val_loss: 0.0752 - val_acc: 0.9774\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0239 - acc: 0.9923 - val_loss: 0.0624 - val_acc: 0.9806\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0189 - acc: 0.9939 - val_loss: 0.0780 - val_acc: 0.9787\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0119 - acc: 0.9962 - val_loss: 0.0651 - val_acc: 0.9811\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0720 - val_acc: 0.9820\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0114 - acc: 0.9963 - val_loss: 0.0716 - val_acc: 0.9824\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0106 - acc: 0.9965 - val_loss: 0.0781 - val_acc: 0.9817\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.2023 - acc: 0.9398 - val_loss: 0.1023 - val_acc: 0.9670\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0771 - acc: 0.9763 - val_loss: 0.0794 - val_acc: 0.9739\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0468 - acc: 0.9851 - val_loss: 0.0627 - val_acc: 0.9792\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0307 - acc: 0.9903 - val_loss: 0.0651 - val_acc: 0.9800\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0210 - acc: 0.9936 - val_loss: 0.0764 - val_acc: 0.9772\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0157 - acc: 0.9952 - val_loss: 0.0580 - val_acc: 0.9840\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0166 - acc: 0.9944 - val_loss: 0.0672 - val_acc: 0.9796\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0107 - acc: 0.9966 - val_loss: 0.0765 - val_acc: 0.9800\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0096 - acc: 0.9971 - val_loss: 0.0616 - val_acc: 0.9834\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0114 - acc: 0.9964 - val_loss: 0.0846 - val_acc: 0.9807\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 0.2014 - acc: 0.9399 - val_loss: 0.1001 - val_acc: 0.9702\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0747 - acc: 0.9775 - val_loss: 0.0736 - val_acc: 0.9763\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0459 - acc: 0.9851 - val_loss: 0.0642 - val_acc: 0.9803\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0315 - acc: 0.9901 - val_loss: 0.0601 - val_acc: 0.9818\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0220 - acc: 0.9928 - val_loss: 0.0750 - val_acc: 0.9775\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0172 - acc: 0.9943 - val_loss: 0.0710 - val_acc: 0.9780\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0138 - acc: 0.9956 - val_loss: 0.0633 - val_acc: 0.9818\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0119 - acc: 0.9959 - val_loss: 0.0661 - val_acc: 0.9828\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0110 - acc: 0.9964 - val_loss: 0.0738 - val_acc: 0.9802\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0080 - acc: 0.9974 - val_loss: 0.0901 - val_acc: 0.9774\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.2048 - acc: 0.9398 - val_loss: 0.1060 - val_acc: 0.9686\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0755 - acc: 0.9768 - val_loss: 0.0865 - val_acc: 0.9725\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0473 - acc: 0.9847 - val_loss: 0.0659 - val_acc: 0.9804\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0306 - acc: 0.9905 - val_loss: 0.0583 - val_acc: 0.9819\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.0220 - acc: 0.9933 - val_loss: 0.0687 - val_acc: 0.9790\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0166 - acc: 0.9946 - val_loss: 0.0731 - val_acc: 0.9792\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0139 - acc: 0.9958 - val_loss: 0.0649 - val_acc: 0.9828\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0105 - acc: 0.9967 - val_loss: 0.0737 - val_acc: 0.9804\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0097 - acc: 0.9969 - val_loss: 0.0706 - val_acc: 0.9831\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0112 - acc: 0.9962 - val_loss: 0.0819 - val_acc: 0.9781\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2028 - acc: 0.9397 - val_loss: 0.0940 - val_acc: 0.9714\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0743 - acc: 0.9770 - val_loss: 0.0742 - val_acc: 0.9771\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0457 - acc: 0.9860 - val_loss: 0.0642 - val_acc: 0.9791\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0311 - acc: 0.9898 - val_loss: 0.0645 - val_acc: 0.9818\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0234 - acc: 0.9926 - val_loss: 0.0656 - val_acc: 0.9811\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0161 - acc: 0.9952 - val_loss: 0.0712 - val_acc: 0.9792\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0152 - acc: 0.9951 - val_loss: 0.0596 - val_acc: 0.9822\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0119 - acc: 0.9961 - val_loss: 0.0598 - val_acc: 0.9835\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0088 - acc: 0.9973 - val_loss: 0.0974 - val_acc: 0.9748\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0117 - acc: 0.9964 - val_loss: 0.0770 - val_acc: 0.9829\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6Km2mW34ECuk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Ответы для формы"
      ]
    },
    {
      "metadata": {
        "id": "6rrnWDJBECul",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ответом для формы должно служить максимальное качество на тестовой выборке. Поскольку в keras результаты разнятся от запуска к запуску, правильный ответ будет засчитан как интервал"
      ]
    },
    {
      "metadata": {
        "id": "6EsYKu5NECum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85439f82-59e3-4ca1-b1e1-a0785bd37f39"
      },
      "cell_type": "code",
      "source": [
        "max_results = max([max(result) for result in results])\n",
        "print('{:.4f}'.format(max_results))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9981\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jwj2isrqfsKI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}